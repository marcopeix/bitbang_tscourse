{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.arima import ARIMASummary\n",
    "from statsforecast.models import AutoARIMA, SimpleExponentialSmoothingOptimized, Holt, HoltWinters, SeasonalNaive\n",
    "\n",
    "from utilsforecast.losses import *\n",
    "from utilsforecast.evaluation import evaluate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"NIXTLA_ID_AS_COL\"] = \"true\"\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (9,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/marcopeix/AppliedTimeSeriesForecastingInPython/refs/heads/master/data/monthly-milk-production-pounds.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df['Month'] = pd.to_datetime(df['Month']+'-01')+pd.offsets.MonthEnd(1)\n",
    "df['Month'] = df['Month'].dt.date\n",
    "df = df.rename(columns={\"Month\": \"Date\", \"Monthly milk production (pounds per cow)\": \"y\"})\n",
    "df.insert(0, 'unique_id', 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df['Date'], df['y'])\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Milk production (lbs/cow)')\n",
    "ax.set_title('Monthly milk production in Australia')\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "\n",
    "# SARIMA\n",
    "\n",
    "sf = StatsForecast(models=[arima, sarima], freq='M')\n",
    "cv_df = sf.cross_validation(h=12, \n",
    "                            df=df, \n",
    "                            n_windows=5, \n",
    "                            step_size=12, \n",
    "                            level=[80], \n",
    "                            time_col='Date')\n",
    "\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df['Date'], df['y'])\n",
    "ax.plot(cv_df['Date'], cv_df['ARIMA'], label='ARIMA')\n",
    "ax.plot(cv_df['Date'], cv_df['SARIMA'], label='SARIMA')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Milk production (lbs/cow)')\n",
    "ax.set_title('Monthly milk production in Australia')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = cv_df.drop(['Date', 'cutoff'], axis=1)\n",
    "evaluation = evaluate(df=eval_df, metrics=[mae, smape])\n",
    "avg_evaluation = evaluation.drop(['unique_id'], axis=1).groupby('metric').mean().reset_index()\n",
    "avg_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = StatsForecast(models=[arima, sarima], freq='M')\n",
    "sf.fit(df=df,time_col='Date')\n",
    "\n",
    "print(ARIMASummary(sf.fitted_[0, 0].model_))\n",
    "print(ARIMASummary(sf.fitted_[0, 1].model_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingUsingFoundationModels/refs/heads/main/data/walmart_sales_small.csv\"\n",
    "\n",
    "df = pd.read_csv(url, parse_dates=[\"Date\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Store', 'Date', 'Weekly_Sales', 'Holiday_Flag']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax = AutoARIMA(max_p=5,\n",
    "                    max_q=5,\n",
    "                    max_d=2,\n",
    "                    max_P=2,\n",
    "                    max_Q=2,\n",
    "                    max_D=1,\n",
    "                    start_p=1,\n",
    "                    start_q=1,\n",
    "                    start_P=0,\n",
    "                    start_Q=0,\n",
    "                    season_length=1)\n",
    "\n",
    "sf = StatsForecast(models=[sarimax], freq='W')\n",
    "cv_df = sf.cross_validation(h=8, \n",
    "                            df=df, \n",
    "                            n_windows=10, \n",
    "                            step_size=8, \n",
    "                            level=[80], \n",
    "                            time_col='Date', \n",
    "                            id_col='Store', \n",
    "                            target_col='Weekly_Sales')\n",
    "\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = cv_df.drop(['Date', 'cutoff'], axis=1)\n",
    "evaluation = evaluate(df=eval_df, metrics=[mae, smape], target_col='Weekly_Sales', id_col='Store')\n",
    "avg_evaluation = evaluation.drop(['Store'], axis=1).groupby('metric').mean().reset_index()\n",
    "avg_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n",
    "\n",
    "uids = Y_df['unique_id'].unique()[:8] # Select 10 ids to make the example faster\n",
    "Y_df = Y_df.query('unique_id in @uids')\n",
    "Y_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster\n",
    "\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12,9))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    uid = uids[i]\n",
    "    plot_df = Y_df[Y_df['unique_id'] == uid]\n",
    "\n",
    "    ax.plot(plot_df['ds'], plot_df['y'])\n",
    "    ax.set_title(f'{uid}')\n",
    "    ax.set_xlabel('Time steps')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set horizon\n",
    "\n",
    "# Seasonal naive\n",
    "\n",
    "# SES\n",
    "\n",
    "sf = StatsForecast(models=[seasonal_naive, ses], freq=1)\n",
    "\n",
    "cv_df = sf.cross_validation(h=h,\n",
    "                            df=Y_df,\n",
    "                            n_windows=3,\n",
    "                            step_size=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12,9))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    uid = uids[i]\n",
    "    plot_df = Y_df[Y_df['unique_id'] == uid]\n",
    "    preds_df = cv_df[cv_df['unique_id'] == uid]\n",
    "\n",
    "    ax.plot(plot_df['ds'], plot_df['y'])\n",
    "    ax.plot(preds_df['ds'], preds_df['SeasonalNaive'], label='Seasonal naive')\n",
    "    ax.plot(preds_df['ds'], preds_df['SES'], label='SES')\n",
    "    ax.set_title(f'{uid}')\n",
    "    ax.set_xlabel('Time steps')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend(loc=2)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DES (Holt)\n",
    "\n",
    "\n",
    "sf = StatsForecast(models=[seasonal_naive, ses, des], freq=1)\n",
    "\n",
    "cv_df = sf.cross_validation(h=h,\n",
    "                            df=Y_df,\n",
    "                            n_windows=3,\n",
    "                            step_size=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12,9))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    uid = uids[i]\n",
    "    plot_df = Y_df[Y_df['unique_id'] == uid]\n",
    "    preds_df = cv_df[cv_df['unique_id'] == uid]\n",
    "\n",
    "    ax.plot(plot_df['ds'], plot_df['y'])\n",
    "    ax.plot(preds_df['ds'], preds_df['SeasonalNaive'], label='Seasonal naive')\n",
    "    ax.plot(preds_df['ds'], preds_df['Holt'], label='DES')\n",
    "    ax.set_title(f'{uid}')\n",
    "    ax.set_xlabel('Time steps')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend(loc=2)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TES (Holt-Winters)\n",
    "\n",
    "\n",
    "sf = StatsForecast(models=[seasonal_naive, ses, des, tes], freq=1)\n",
    "\n",
    "cv_df = sf.cross_validation(h=h,\n",
    "                            df=Y_df,\n",
    "                            n_windows=3,\n",
    "                            step_size=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12,9))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    uid = uids[i]\n",
    "    plot_df = Y_df[Y_df['unique_id'] == uid]\n",
    "    preds_df = cv_df[cv_df['unique_id'] == uid]\n",
    "\n",
    "    ax.plot(plot_df['ds'], plot_df['y'])\n",
    "    ax.plot(preds_df['ds'], preds_df['SeasonalNaive'], label='Seasonal naive')\n",
    "    ax.plot(preds_df['ds'], preds_df['HoltWinters'], label='TES')\n",
    "    ax.set_title(f'{uid}')\n",
    "    ax.set_xlabel('Time steps')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend(loc=2)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = cv_df.drop(['ds', 'cutoff'], axis=1)\n",
    "evaluation = evaluate(df=eval_df, metrics=[mae, smape])\n",
    "avg_evaluation = evaluation.drop(['unique_id'], axis=1).groupby('metric').mean().reset_index()\n",
    "avg_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitbang-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
